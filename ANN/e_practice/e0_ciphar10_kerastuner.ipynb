{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5751118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff4fa72",
   "metadata": {},
   "source": [
    "## ğŸ§  What is **KerasTuner**?\n",
    "\n",
    "**KerasTuner** is a library for **hyperparameter optimization** for Keras models. Instead of manually searching or using `GridSearchCV`, it automates finding the best model by trying many configurations.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Why KerasTuner?\n",
    "\n",
    "- Built specifically for **Keras/TensorFlow**\n",
    "- Supports **random search**, **Bayesian optimization**, and **Hyperband**\n",
    "- Better scalability and visualization compared to `GridSearchCV`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ How it works â€” Steps:\n",
    "\n",
    "1. **Define a model-building function**  \n",
    "   This function includes *searchable hyperparameters* using `hp` (HyperParameters object).\n",
    "\n",
    "2. **Select a tuner**  \n",
    "   Choose the strategy: `RandomSearch`, `BayesianOptimization`, `Hyperband`, etc.\n",
    "\n",
    "3. **Search for best hyperparameters**  \n",
    "   Call `.search()` to start the process.\n",
    "\n",
    "4. **Access best model or params**  \n",
    "   Use `.get_best_models()` or `.get_best_hyperparameters()`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ What can be asked in exams?\n",
    "\n",
    "1. **What is KerasTuner?**\n",
    "2. **Why use it over GridSearchCV?**\n",
    "3. **What types of tuners are available?**\n",
    "4. **What is a model-building function?**\n",
    "5. **How does it handle learning rate, dropout, etc.?**\n",
    "6. **How do you retrieve the best model and hyperparameters?**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ed89b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 07m 25s]\n",
      "val_accuracy: 0.4284000098705292\n",
      "\n",
      "Best val_accuracy So Far: 0.48089998960494995\n",
      "Total elapsed time: 01h 09m 05s\n",
      "\n",
      "âœ… Best Hyperparameters Found:\n",
      "Learning Rate: 0.0005\n",
      "Dropout1: 0.3\n",
      "Dropout2: 0.2\n",
      "Units1: 256\n",
      "Units2: 256\n",
      "Epoch 1/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.2794 - loss: 2.1855 - val_accuracy: 0.3572 - val_loss: 1.8309\n",
      "Epoch 2/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9640s\u001b[0m 15s/step - accuracy: 0.3694 - loss: 1.7867 - val_accuracy: 0.3045 - val_loss: 2.0214\n",
      "Epoch 3/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.3913 - loss: 1.7197 - val_accuracy: 0.4141 - val_loss: 1.6648\n",
      "Epoch 4/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.4074 - loss: 1.6692 - val_accuracy: 0.4198 - val_loss: 1.6435\n",
      "Epoch 5/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.4085 - loss: 1.6496 - val_accuracy: 0.4467 - val_loss: 1.5572\n",
      "Epoch 6/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.4272 - loss: 1.6188 - val_accuracy: 0.4333 - val_loss: 1.6032\n",
      "Epoch 7/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.4197 - loss: 1.6128 - val_accuracy: 0.4543 - val_loss: 1.5422\n",
      "Epoch 8/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.4353 - loss: 1.5810 - val_accuracy: 0.4618 - val_loss: 1.5141\n",
      "Epoch 9/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 44ms/step - accuracy: 0.4364 - loss: 1.5796 - val_accuracy: 0.4398 - val_loss: 1.5643\n",
      "Epoch 10/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 20ms/step - accuracy: 0.4457 - loss: 1.5641 - val_accuracy: 0.4469 - val_loss: 1.5418\n",
      "Epoch 11/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.4464 - loss: 1.5538 - val_accuracy: 0.4282 - val_loss: 1.5998\n",
      "Epoch 12/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.4476 - loss: 1.5450 - val_accuracy: 0.4396 - val_loss: 1.5623\n",
      "Epoch 13/40\n",
      "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 22ms/step - accuracy: 0.4497 - loss: 1.5393 - val_accuracy: 0.4285 - val_loss: 1.5977\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4759 - loss: 1.4818\n",
      "\n",
      "ğŸ¯ Test Accuracy: 0.4723, Test Loss: 1.4857\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# 1. Load and preprocess CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# 2. Define the model building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(32, 32, 3)))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(units=hp.Choice(\"units_1\", [256, 512]), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=hp.Choice(\"dropout_1\", [0.3, 0.4])))\n",
    "\n",
    "    model.add(Dense(units=hp.Choice(\"units_2\", [128, 256]), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=hp.Choice(\"dropout_2\", [0.2, 0.3])))\n",
    "\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    lr = hp.Choice(\"learning_rate\", [0.001, 0.0005, 0.0001])\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 3. Initialize the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='keras_tuner_cifar10',\n",
    "    project_name='cifar10_ann'\n",
    ")\n",
    "\n",
    "# 4. Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train,\n",
    "             validation_split=0.2,\n",
    "             epochs=20,\n",
    "             batch_size=64,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)])\n",
    "\n",
    "# 5. Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "print(\"\\nâœ… Best Hyperparameters Found:\")\n",
    "print(\"Learning Rate:\", best_hps.get('learning_rate'))\n",
    "print(\"Dropout1:\", best_hps.get('dropout_1'))\n",
    "print(\"Dropout2:\", best_hps.get('dropout_2'))\n",
    "print(\"Units1:\", best_hps.get('units_1'))\n",
    "print(\"Units2:\", best_hps.get('units_2'))\n",
    "\n",
    "# 6. Build and train the best model\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# 7. Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nğŸ¯ Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7c892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
